### Abstraction

巨大的模型限制了引用他们到复杂环境，包括：存在极大的数据空间，存储空间和计算资源有限的情况下。

因此，本文提出了一种压缩模型的方法。

### Introduction

使用ensemble methods（集成学习）的方法往往面临着参数过多，模型过大的问题，以至于这些方法不能被应用于资源受限的场景下。

本文提出了一种通过紧凑的“人工神经网络”来拟合ensemble methods，通过更少的参数来达到类似的效果。其中很主要的一点是，要通过已完成的ensemble methods来扩大数据集，使得神经网络可以有更大的训练数据集。

当然，大数据集并不总是可行的，在一些domain，我们无法获取大数据集，本文也提出一种称之为MUNGE的方法来获得和原始数据集同分布的“假数据”来达到我们的目的。

### Model Compression

 精确的模型往往在速度和规模上不尽如人意，所以在很多应用场景下，我们都希望能够将模型压缩，能够使用更小，更快的模型完成一些任务，同时尽可能保证模型的精度损失是几乎可以被忽略的。

一般来说，模型压缩的核心思想就是：**使用更简单的模型去拟合一个训练好的复杂模型**。

一种在当时很好的思路是，使用紧凑的人工网络去拟合集成学习的模型。但这面临着一个问题，如何扩充数据集？本文中给出三种方法的比较：

- Random

  从每个属性的边缘分布中独立的抽样该属性。方法是简单朴素的，但面临的问题是这样的采样会破坏属性之间的关联，某种意义上让新的数据和原有数据并不是同意分布

- Naive Bayes Estimation  

  本质来说，是将原来有的数据分布看做一个正态分布，然后从中重新采样，生成新的数据。

  朴素贝叶斯是一种方法。

- MUNGE

  是一种从最邻近邻居着手的方法。





