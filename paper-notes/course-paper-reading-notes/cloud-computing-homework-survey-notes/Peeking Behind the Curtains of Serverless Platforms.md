### Introductions

Go behind the scenes of a serverless system with performance testing.

云计算使得后端基础架构维护与应用程序开发越来越脱钩。无服务器计算(或称为“服务即服务”( Function-as-a-service，FaaS )是一种新兴的应用程序部署架构，它完全隐藏了租户对服务器的管理(因此命名)。租户接受应用程序的运行时配置的最小访问。这使得租户可以专注于开发自己的功能- -专门用于特定任务的小型应用程序。一个函数通常在具有CPU时间和内存等受限资源的专用函数实例( 一个容器或其他类型的沙箱 )中执行。与更传统的架构即服务 ( IaaS )平台中的虚拟机( VM )不同，函数实例只有在调用函数并在处理请求后立即入睡时才会启动。租户按每次调用的方式收取费用，无需支付未使用和闲置的资源。

无服务器计算起源于处理低占空比工作负载的设计模式，例如响应存储在云上的文件的不频繁更改而进行处理。现在它被用作各种应用程序的简单编程模型[  14,22,42 ]。隐藏租户的资源管理使这种编程模型得以实现，但由此产生的不透明性妨碍了许多潜在用户的采用，这些潜在用户对此表示了关切：隔离质量方面的安全、拒绝服务攻击，以及更多[  23,35,37,40 ]；了解资源管理以提高应用程序性能的必要性[  4、 19、 24、 27、 28、 40 ]；以及平台传递性能的能力[ 10-12，29-31 ]。虽然有人试图为平台的资源管理和安全提供启示[  33,34 ]，但正如我们将要表明的那样，已知的测量技术未能提供准确的结果。

因此，我们在三个流行的无服务器计算提供商：AWS Lambda、Azure Functions和Google Cloud Functions ( GCF )中对资源管理和性能隔离进行了迄今为止最深入的研究。我们首先使用量测驱动方法对Lambda和Azure函数的体系结构进行部分逆向工程，揭示了许多无文档的细节。然后，我们系统地研究了与资源管理相关的一系列问题：**如何快速地启动函数实例**、函数实例放置策略、函数实例重用等。识别并讨论了几个安全问题。1引言我们进一步探讨了CPU、I / O和网络带宽如何在功能间分配以及随之而来的性能含义。最后但并非最不重要的是，我们探讨所有资源是否被恰当地核算，并报告两个允许租户免费使用额外资源的资源核算漏洞。我们的结果中的一些亮点包括：

1. AWS Lambda实现了最好的可扩展性和最低的冷起动延迟(提供新函数实例的时间)，其次是GCF。但在AWS中，来自同一帐户的函数实例之间缺乏性能隔离，导致I / O、联网或冷起动性能下降了19倍。
2. Azure Functions使用不同类型的VM作为主机：55 %的时间函数实例运行在性能下降的VM上。
2. Azure存在可利用的安放漏洞：租户可以安排函数实例运行在与另一个租户相同的VM上，这是跨函数侧信道攻击的敲门砖。
2. GCF中的一个会计核算问题使一个人能够使用函数实例以几乎不花费任何代价实现与小型VM实例相同的计算资源

体内还给出了许多结果。我们在2018年5月重复了几次测量，并在文中突出了提供商所做的改进。我们注意到无服务器平台正在迅速发展；尽管如此，我们的研究结果提供了流行的无服务器平台资源管理机制和效率的快照，为开发人员建立更可靠的平台提供了性能基准和设计方案，并帮助租户改进对无服务器平台的使用。更一般地说，我们的研究提供了对其他研究者有用的新的测量技术。为了便于实现，我们将使我们的测量代码公开和开源。

-----

无服务器计算平台。在无服务器计算中，应用程序通常由一个或多个功能组成- -独立的、小型的、无状态的组件来处理特定的任务。一个函数最常由用某种脚本语言编写的小块代码来指定。无服务器计算提供商对功能的执行环境和后台服务器进行管理，并动态分配资源，保证其可扩展性和可用性。近年来，包括亚马逊、Azure、谷歌、IBM在内的云提供商开发和部署了许多无服务器计算平台。我们关注的是亚马逊AWS Lambda、Azure Functions和Google Cloud Functions。

在这些服务中，一个是在资源有限的专用容器或其他类型的沙箱中执行的。我们使用函数实例引用函数运行的容器/沙箱。功能实例所宣传的资源因平台而异，如表1所示。当函数被请求调用时，将启动一个或多个函数实例(视请求量而定)来执行函数。当函数实例( s )处理请求并退出或达到最大执行时间( 见表 1中的“超时 )后，函数实例( s )变为空闲。它们可以被重用来处理后续的请求，以避免新实例的延迟启动。但是，空闲函数实例也可以突然终止。

每个函数实例与一个非持久的本地磁盘相关联，用于临时存储数据，当函数实例被破坏时，该磁盘将被删除。使用无服务器服务的一个好处是租户在函数实例空闲时不支付所消耗的资源。租户只在执行过程中根据资源消耗进行计费。4跨平台的共同特点是对所有调用的聚合函数执行时间进行计费。

此外，价格取决于预先配置的函数内存(  AWS ,谷歌 )或调用期间实际消耗的内存( Azure )。谷歌进一步根据CPU速度收取不同的费率。

----

我们以无服务器用户的观点来刻画无服务器平台的体系结构、性能和资源管理效率。我们在同一云提供者区域设置优势点，通过官方API从一个或多个账户中管理和调用函数，并利用函数可用的信息确定重要特征。我们在各种设置下重复同样的实验，通过调整功能配置和工作量来确定可能影响测量结果的关键因素。在论文的其馀部分，我们只报告了影响实验结果的相关因素
我们将所有必要的功能和子程序集成到一个我们称之为测量函数的单一函数中。一个测量函数执行两个任务：( 1 )收集调用时序和函数实例运行时信息，( 2 )根据接收到的消息运行指定的子程序(如测量本地磁盘I/O吞吐量、网络吞吐量)。测量函数通过Linux上的Pro文件系统( profs )、环境变量和系统命令收集运行时信息。它还报告执行开始和结束时间、调用ID (由唯一标识一个调用的函数生成的随机16字节ASCII字符串)以及函数配置，以方便进一步分析。
该度量函数在本地磁盘上检查一个名为InstanceID的文件是否存在，如果不存在，则用一个随机的16字节ASCII字符串创建该文件作为函数实例ID。由于本地磁盘是非持久性的，并且与关联的函数实例具有相同的生存期，因此InstanceID文件对于一个新鲜的函数实例不会存在，一旦创建，在函数实例生存期内不会被修改或删除
功能区域分别为AWS、Google和Azure (分别为us-east-1、us-center-1、‘EASTUS’。优点是VMs至少有4 GB RAM和2 vCPU。我们使用了提供商推荐的软件，并遵循官方指令在优势点配置时间同步服务。5我们在各种语言中实现了测量功能，但大多数实验都使用Python 2.7和Nodejs 6 . 作为语言运行时( Newrelic 认为AWS中最流行的前2种语言)。我们通过同步的HTTP请求来调用这些函数。我们的大部分测量是从2017年7月至12月完成的

伦理方面的考虑。我们以不应给平台或其他租户造成不应有负担的方式来构建我们的计量功能。在大多数实验中，该功能只不过是收集必要的信息和睡眠一定时间。一旦我们发现了性能问题，我们将测试限制在其他租户身上。我们只是进行了小规模的测试来检查安全问题，但没有进一步利用这些问题

---------------
我们结合两种方法推断出AWS Lambda、Google Cloud Functions和Azure Functions的体系结构：( 1 )回顾官方文件、相关在线文章和讨论，( 2 )测量——在不同条件下多次运行我们的测量函数(  > 50 000 )所收集的数据。该数据使得AWS、Azure和Google的体系结构能够部分逆向工程。
AWS。函数在专用函数实例中执行。我们的度量表明，不同版本的函数将被视为不同的，并在不同的函数实例中执行(我们讨论x5.5中的离群值)。profs文件系统公开底层VM主机的全局统计信息，而不仅仅是一个函数实例，并包含用于分析运行时、识别主机VM等有用信息。从profs中我们发现主机VM大多有2个vCPU和3.75GB物理RAM ( 与 EC2c4大实例相同 )。
Azure。Azure函数使用函数应用程序来组织函数。一个功能应用程序对应一个功能实例，是一个包含单个功能执行环境的容器。函数实例中的环境变量包含有关主机VM的一些全局信息。采集的环境变量表明宿主虚拟机可以有1、2或4个vCPU。一个应用程序可以在一个函数应用程序中创建多个函数并同时运行。在我们的实验中，我们假设一个函数应用程序只有一个函数。
谷歌。谷歌分离并过滤可以从profs访问的信息。Profs下的文件只报告当前函数实例的使用统计。此外，许多系统文件和syscalls被遮蔽或禁用，因此我们无法获得有关运行时的许多信息。/ proc / meminfo和/ proc / cpuinfo文件显示一个功能实例有2 GB RAM和8个vCPU，我们猜想这是对虚拟机的配置

将函数实例与VM联系起来，可以使我们执行更丰富的分析。Lloyd等人提出的在AWS Lambda中识别VM的启发式方法虽然在理论上是可能的，但从未被实验评估过。因此，我们寻找一种更稳健的方法。
AWS。/ pro / self / cgroup文件有一个特殊的条目，我们称之为实例根ID。它从" sandboxroot - "开始，然后是一个6字节的随机字符串。我们发现它可以用来可靠地标识一个主机VM。使用基于I / O的coresidency测试( 如图 3所示 )，我们证实共享同一个实例根ID的实例在同一个VM上，因为连续两次调用之间写入的总字节的差异，对于fi和fi + 1分别来说，与fi写入的字节数几乎相同。此外，当同时读取/Pro / uptime ( /Pro / meminfo )时，我们可以从实例中获得相同的内核上升时间( 或内存使用统计 )。我们调用实例VM公共IP通过查询IP地址查找工具获得的IP，以及运行uname命令VM私有IP获得的IP。共享同一个实例根ID的函数实例具有相同的VM公共IP和VM私有IP
Azure。根据官方文件，WEBSITE INSTANCE ID环境变量作为VM标识符。我们将其称为Azure VM ID。我们使用共享DLL的Flush - Reload来验证共享相同Azure VM ID的实例的核心驻留性。结果提示Azure VM ID是一个健壮的VM标识符。
谷歌。我们无法找到任何能够识别主机的信息。使用基于I / O的核心轮值并不起作用，因为procfs不包含全局使用统计。我们尝试使用性能作为隐蔽通道(例如，在一个函数实例中执行模式化的I/O操作，在另一个函数实例中从I/O吞吐率的变化中检测模式)，但发现这并不可靠，因为性能变化很大(见x6.2 )。

前期研究表明，在AWS EC2中共定位VM允许攻击[  36、 38、 41 ]。借助实例—VM关系的知识，我们考察了租户的原生资源—功能实例—是如何孤立的。我们假设一个租户对应一个用户帐户，只考虑VM级的核心主席。
啊。由同一个租户创建的函数将共享同一组VM，不管它们的配置和代码如何。详细的实例放置算法将在x5.1中讨AWS。AWS给每个租户分配不同的VM，因为我们从未在同一个VM中看到来自不同租户的函数实例。为了验证这一假设，我们进行了一个跨租户的核心成员测试。基本原理与图3类似：在每一轮中，我们同时在两个账号的每个下面创建一个新的函数，在一个函数中写一个随机数字节，在另一个函数中检查磁盘使用量统计。我们进行了为期1周的测试，但没有发现跨租户函数实例的VM-coresidency。
Azure,Azure函数是Azure App服务的一部分，所有租户根据Azure 共享相同的虚拟机集合。因此，Azure函数中的租户也应该共享VM资源。一个简单的测试证实了这一假设：我们在两个帐户中每个调用500个函数，发现30 %的函数实例是同时驻留，其中一个函数实例来自另一个帐户，共执行120个VM。注意到截至2018年5月，不同租户在Azure不再共享相同的虚拟机。详情见X5.1。

-----
#### 异质型基础设施
我们发现所有考虑的服务中的VM具有多种配置。该品种很可能是基础设施升级造成的，会造成功能性能不一致。为了估计给定服务中不同类型VM的比例，我们检查了每个服务中5万个唯一函数实例的主机VM的配置。在AWS中，我们检查了/ proc / cpuinfo和/ proc / meminfo中的模型名称和处理器编号，发现了五种类型的虚拟机：两种E5 - 2666 vCPU ( 2.90 GHZ )，两种E5 - 2680 vCPU ( 2.80 GHZ )，两种E5 - 2676 vCPU ( 2.40 GHZ )，两种E5 - 2686 vCPU ( 2.30 GHZ )和一种E5 - 2676 vCPU。这些类型分别占20，447个VM的59.3 %，37.5 %，3.1 %，0.09 %和0.01 %
Azure显示VM配置的多样性更大。Azure中的实例报告了各种vCPU计数：在4，104个独特的VM中，54.1 %使用1个vCPU，24.6 %使用2个vCPU，21.3 %使用4个vCPU。对于给定的vCPU计数，有三种CPU模型：两个Intel和一个AMD。因此，Azure使用了9种(至少)不同类型的虚拟机。性能可能会根据运行该函数的主机类型(更具体地说，vCPU的数量)而有很大差异。详情见X6。在谷歌，模型名称总是‘未知’的，但有4个唯一的模型版本(  79、 85、 63、 45 )，分别对应47.1 %、44.7 %、4.2 %和4.0 %的选定函数实例。

#### 讨论
能够识别AWS中的VM对我们的测量至关重要。它有助于减少实验中的噪音，得到更精确的结果。为了比较，我们对Lloyd等人设计的启发式进行了评估。。启发式假设不同的VM具有不同的引导时间，可以从/ pro / stat和基于引导时间的组函数实例中获取。我们将10 - 50个并发请求一次发送到1536个MB函数进行100轮，使用我们的方法( 实例根 ID + IP )对虚拟机进行标记，并与启发式方法进行比较。该启发式将940个VM确定为600个VM，因此340个VM ( 36 % )被错误标记。所以，我们断定这种启发式是不可靠的
这些无服务器的提供者都没有完全从租户那里隐藏运行时信息。更多的实例运行时和后端基础设施的知识可以使攻击者更容易在功能实例中发现漏洞。在以前的研究中，profs被用作侧通道[  9、 21、 46 ]。在无服务器的设置中，实际上可以使用它来监视同时驻留实例的活动；虽然看似无害，但一个专注的对手可能会利用它作为更复杂攻击的基石。总体而言，除非有必要，为安全目的，应限制对运行时信息的访问。此外，提供者应以可审计的方式公开这些信息，即通过API调用，从而能够检测和阻止可疑行为。

-----
### 资源调度
我们从实例冷起动延迟、生存期、可扩展性等方面考察了实例和VM在3个无服务器平台中是如何被调度的。
#### 可扩展性和实例放置
弹性的自动缩放以响应需求的变化是无服务器模型的主要优点。我们衡量平台规模有多大。我们创建了40个相同内存大小f1的测量函数；F2；：：：f40并以5i个并发请求调用每个fi。为了应对平台中的速率限制，我们在批调用之间停顿了10秒。所有测量功能只需睡眠15秒，然后返回。对于每个配置，我们执行了50轮测量。
AWS。在支持并发执行的三个服务中，AWS是最好的。在我们的度量中，N个并发调用总是产生N个并发运行的函数实例。AWS可以很容易地扩展到200个(最大测量并发级别)的新鲜函数实例。
我们观察到3，328MB是AWS Lambda中任意VM上所有函数实例都可以分配的最大聚合内存。AWS Lambda似乎将实例放置视为一个binpacking问题，并试图在现有的活动VM上放置一个新的函数实例，以最大化VM内存利用率，即实例内存大小之和除以3，328。我们用一组并发请求调用单个函数，以5步的步长从5增加到200，并记录每个请求数后被使用的VM总数。几个例子如图4所示。
如果AWS最大化VM内存利用率，则活跃VM的数量接近‘期望’数量。从数量上讲，我们在测试中得到的89 %以上的VM实现了100 %的内存利用率。向不同的函数发送并发请求导致了相同的模式，表明放置对函数代码是不确定的。在进一步的测试中，我们将10组并发请求的随机数发送给随机选择的内存大小在50次以上的函数。AWS的放置仍然有效：同一运行中跨VM的平均VM内存利用率为84.6 % ~ 100 %，中位数为96.2 %。
Azure。Azure文档表示，对于一个基于Nodejs的函数，它将自动扩展到最多200个实例，并且最多每10秒就可以启动一个新的函数实例。然而，在我们对Nodejs基函数的测试中，不管我们如何更改调用之间的间隔，我们最多看到10个函数实例同时为单个函数运行。所有的请求都由一组小的函数实例来处理。所有并发运行的实例都不在同一个VM上。所以，看来Azure并不试图将同一函数的函数实例在同一VM上进行协同定位。我们进行了一个单帐户核心主机测试，以检查函数实例是如何放置在不同数量vCPU的VM上的。我们一次从一个帐户调用100个不同的函数，直到有1000个并发的、截然不同的函数实例运行。然后我们检查了同驻，并重复了整个测试10次。
我们在单个1 / 2 / 4 - vCPU VM上最多观察到8个实例。共驻实例往往位于1 - vCPU虚拟机上(可能是因为Azure函数有更多的1 - vCPU虚拟机)。我们将核心总统任期结果分解见表5。一般来说，对于需要多个功能实例的用户来说，协同驻留是不可取的，因为低端VM上实例之间的争执会加剧性能问题
我们进一步在更现实的场景中进行了跨帐户核心主机测试，攻击者希望将其函数实例与目标受害者实例放在同一VM上。在每一轮测试中，我们分别从一个帐户(受害者)和另一个帐户(攻击者)发起了5或100个函数实例和500个同时发起的函数实例。平均而言，500个攻击者实例中0.12 % ( 3.82 % )为同时驻留，每轮受害者实例为5 ( 100 )个( 共计 10轮 )。因此，即使对于少数目标，也有可能实现跨租户的核心任期。在使用100个受害者实例的测试中，我们可以在同一个VM上获取多达5个攻击者实例。安全影响将在x5.6中讨论
我们在2018年5月重复了核心轮值测试，但没有发现任何跨租户的同时驻留实例，即使在测试中我们累了500个受害者实例。因此，我们认为阿祖尔解决了跨租户的核心总统问题。
谷歌。Google未能提供我们期望的可扩展性，尽管Google声称HTTP触发的功能会迅速扩展到期望的调用速率。通常情况下，即使在低并发级别( 例如 , 10 )的情况下，只有大约一半的期望实例可以同时启动，而剩馀的请求则被排队。

#### 冷起动和VM提供
我们使用冷起动来参考一个新的函数实例的启动过程。对于平台而言，一个冷起动可能涉及启动一个新的容器、设置运行时环境、部署一个函数，这将比重用一个现有的函数实例( hotstart )需要更多的时间来处理请求。因此，冷起动s能够显著影响应用响应性，进而影响用户体验。对**于每个平台，我们创建了1，000个相同内存和语言的不同函数，并依次调用其中的每个函数两次来收集其冷起动和温启动延迟**。我们使用调用发送时间( 由优势点记录 )和函数执行开始时间(由函数记录)的差异作为其冷起动 /温启动延迟的估计。作为基线，在所有调用中，AWS、Google和Azure的温启动延迟中值分别约为25、79和320 ms。
AWS  我们检查了两类冷起动事件：**函数实例是在一个以前从未见过的新VM上启动的( 1 )和( 2 )在现有VM上启动的**。直觉上，case ( 1 )应该比( 2 )有更长的冷起动延迟，因为case ( 1 )可能涉及启动新VM。然而，我们发现**病例( 1 )一般只比( 2 )稍长。病**例( 1 )的冷起动潜伏期中位数仅比( 2 )长39ms (所有设置)。此外，我们发现的最小VM内核上升时间( from / pro / uptime )为132秒，**表明VM在调用之前已经启动。**
因此，**AWS有一个准备好的VM池**。例( 1 )中的额外延迟更可能是通过调度(例如选择一个VM )而不是启动一个VM引入的。我们的结果与之前的观察一致：**功能记忆和语言影响冷起动潜伏期**，如图6所示。Python2.7的平均冷起动延迟最低( 167 - 171 ms )，而Java函数的延迟明显高于其他语言( 824 - 974 ms )。冷起动延迟一般随着函数内存的增加而减小。一种可能的解释是AWS按内存大小比例分配CPU功率；随着CPU功率的增加，建立的环境变得更快(见x6.1 )
**由于AWS的实例放置策略，多个函数实例可能同时在同一VM上启动。**在这种情况下，冷起动延迟会随着更多实例同时启动而增加。例如，在给定的VM上启动一个具有128 MB内存的Python 2.7函数的20个函数实例，平均耗时1 321 ms，比在同一VM上启动1个函数实例( 186 ms )慢约7倍。
Azure和Google。谷歌的冷起动潜伏期中位数从110 ms到493 ms不等(见表7 )。谷歌也按内存比例分配CPU，但在谷歌内存大小对冷起动延迟的影响要比在AWS中大。**在Azure中启动一个函数实例需要花费更长的时间，尽管它们的实例总是被分配1.5 GB的内存。Azure的冷起动潜伏期中位数为3640 ms**。网上的轶事表明，长时间的延迟是由Azure意识到并致力于改进的平台中的设计和工程问题造成的。
延迟差异。我们每隔10秒采集128 MB、Python 2.7 ( AWS )或Nodejs 6.2 . * ( Google和Azure )基函数的冷起动延迟超过168小时( 7天)，并计算在给定时间内采集的冷起动延迟的中位数。冷起动延迟的变化如图8所示。AWS的冷起动延迟相对稳定，谷歌的除少数穗外延迟也相对稳定。随着时间的推移，Azure的网络变化最大，从大约1.5秒到16秒不等。
我们在2018年5月重复了冷起动的测量。我们在AWS中没有发现冷起动延迟的显著变化。但是，冷起动延迟在谷歌平均慢了4倍，这可能是由于其2018年2月的基础设施更新，而在Azure则好了15倍。这一结果证明了为无服务器系统开发测量平台(类似于对IaaS )的重要性，以便为更好的性能特性进行连续测量。
#### 实例生命周期
一个无服务器的提供者即使仍在使用中，也可能终止一个函数实例。我们定义函数实例保持活动的最长时间为实例生存期。租户更喜欢长寿命，因为他们的应用程序能够更长地保持内存状态(例如数据库连接)，更少地遭受冷启动。为了估计实例生存期，我们设置了不同内存大小和语言的函数，并以不同的频率调用它们(每5 / 30 / 60秒一次请求)。一个函数实例的寿命是我们第一次看到该实例和最后一次看到该实例的时间之差。我们进行了7天( AWS和Google )或更长时间( Azure )的实验，以便在给定的环境下收集至少50个生命周期
一般来说，Azure函数实例的生存期明显长于AWS和Google，如图9所示。在AWS中，所有设置的中值实例生存期为6.2小时，最大值为8.3小时。AWS中的主机VM通常寿命较长：观察到的最长VM内核上升时间为9.2小时。当请求频率增加时实例寿命趋于变短。除了谷歌，其他因素对生存期的影响不大，因为内存较大的实例往往生存期较长。例如，每5秒调用一次，在谷歌128MB和2，048MB内存的90 %实例中，生存期分别为3 - 31分钟和19 - 580分钟。因此，对于负载较重下内存较小的函数，谷歌似乎更积极地推出新实例，而不是重用已有实例。这样可以增加来自冷启动的性能惩罚

#### 空闲实例循环
为了有效地使用资源，无服务器提供者关闭空闲实例以回收分配的资源(如 )。我们定义一个实例在关闭之前能够保持空闲的最长时间为实例的最大空闲时间。长空闲时间和短空闲时间之间存在权衡，因为维护更多的空闲实例是对VM内存资源的浪费，而更少的准备服务实例会导致更多的冷启动。我们对导致不同函数实例的两个函数调用之间的最小延迟排序进行了二进制搜索。我们创建了一个函数，在1 ~ 120分钟之间以一定的延时两次调用，并确定两个请求是否使用同一个函数实例。我们重复，直到鉴定出tidle。对于接近整齐的延迟，我们重复测量100次，确认整齐(到分钟粒度)。
AWS 一个实例通常最多能保持27min的不活动。事实上，在80 %的轮次中，26分钟后被关机。当它们的主机VM是‘空闲的’，即在那个VM上没有活动实例时，空闲函数实例将以下述方式循环：假设N个函数f1的函数实例；：：：fN是一个VM上的同时驻留，kfi实例来自fi。对于给定的函数fi，AWS将每隔300秒(或更多或更少)关闭fi的空闲实例bkfi = 2c，直到剩下两三个实例，并最终在27分钟后关闭剩下的实例(我们用kfi = 5进行了测试)；10；15；20 )。AWS将这些操作执行到f1；：：：fN独立于给定VM上，也独立于个体VM上。功能记忆或语言不影响最大空闲时间。
如果VM上存在活动实例，实例可以在较长时间内保持不活动。我们通过每10秒发送一次请求，在给定的VM上保持一个实例的活动，发现：( 1 ) AWS仍然采用相同的策略来回收同一函数的空闲实例，但( 2 )对于其他同时驻留实例，不知为何空闲时间被重置。我们观察到在这种情况下，一些空闲实例可以保持空闲1 - 3小时。
Azure和Google。在Azure中，我们无法找到一个一致的最大实例空闲时间。我们在不同的日子重复了几次实验，发现最大空闲时间为22、40、120分钟以上。在谷歌，实例的空闲时间可能超过120分钟。120分钟后，18 %的实验保持活跃。
#### 不一致的函数使用
租户期望一个函数更新后的请求应该由新的函数代码来处理，特别是如果更新是安全关键的。然而，我们在AWS中发现，请求可以通过一个旧版本的函数来处理的可能性很小。我们称这类情况为函数用法不一致。在实验中，我们将k = 1或k = 50的并发请求发送到一个函数中，并且在更新了函数的以下几个方面：IAM角色、内存大小、环境变量或函数代码之后，毫不拖延地再次这样做。对于给定的设置，我们执行了100轮的这些操作。当k = 1时，1 % -4 %的测试使用不一致函数。当更新前关联实例较多( k = 50 )时，我们80 %的轮至少有一个不一致的函数。纵观所有轮次的测试，我们发现3.8 %的实例运行的函数不一致。审查这些案件，我们发现了两种情况
1 ) AWS启动新的过时函数实例( 占所有案件的 ，( 2 ) AWS重用已有过时函数实例。不一致的实例在终止前从未处理过多个请求(注意AWS中最大执行时间为300 s )，但仍然有相当一部分请求可能无法得到预期的结果。由于在函数更新发送请求后等待的时间较长，我们发现不一致的情况较少，最终以6秒的等待时间为零。所以，我们怀疑不一致问题是由实例调度器中的种族条件引起的。结果提示，由于调度器无法进行原子更新，多个函数实例之间的协调功能更新具有挑战性。
#### 讨论
我们相信我们的结果将激励我们进一步研究设计更高效的实例调度算法和健壮的调度器来进一步提高VM资源的利用率，即最大化VM内存的使用量，减少调度延迟，并在保证一致性的同时传播函数更新。在冷起动 [  1、 3 ]期间加载模块或库会引入高延迟。为了减少冷起动延迟，提供者可能需要采用更为复杂的库加载机制，例如，使用库缓存来加快这一进程，并在部署前解决库依赖问题，只加载所需的库
在Azure中共享跨租户VM加上在函数实例中运行任意二进制的能力，可以使应用程序容易受到多种侧信道攻击[  16、 17、 20、 45 ]。我们没有研究Azure如何很好地应对跨租户VM共享带来的潜在威胁，并将实际的安全问题作为一个开放性的问题。AWS的bin-packing放置可能会给一个应用程序带来安全问题，这取决于它的设计。当Lambda中的多租户应用程序使用IAM角色隔离其租户时，来自不同应用租户的函数实例仍然共享相同的VM。我们发现了两个使用这种模式的真正服务：Backand 和Zaier 。两者都允许其租户以某种方式在Lambda中部署功能。我们在Backand仅仅几次尝试就成功实现了跨帐户功能核心区，而在Za庇尔由于其速率限制和庞大的用户基础( 1M + )而失败。
尽管如此，我们仍然可以观察到其他扎皮尔租户的申请所引起的进程的变化，这可能会允许旁通道[  9、 21、 46 ]。对于这些多租户应用程序，为了隔离其租户并实现更好的安全性和隐私性，AWS可能需要提供更细粒度的VM隔离机制，即为每个IAM角色分配一组VM，而不是分配给每个帐户。
### 隔离表现
在本节中，我们研究性能隔离。我们主要关注的是AWS和Azure，在那里，我们实现核心席位的能力允许更精细的测量。我们还给出了一些谷歌中表面看起来与其他租户争执的例子的基本性能统计。
#### CPU利用率
为了测量CPU利用率，我们的测量函数使用时间. time ( Python )或Date.now ( Nodejs )连续记录1，000 ms的时间戳。度量实例CPU利用率定义为记录一个时间戳的1 000 ms的百分比。啊。根据AWS，一个函数实例的CPU功率与其预先配置的内存成正比。然而，AWS并没有详细说明CPU时间如何分配到实例中。我们测量了1，000个不同函数实例的CPU利用率，并显示了图10a中给定内存大小的中值率
内存较高的实例会获得更多的CPU循环。中值实例CPU利用率随着内存从128 MB增加到1536 MB，从7.7 %增加到92.3 %，相应的标准差( SD )分别为0.7 %和8.7 %。当没有来自其他同时驻留实例的争执时，一个实例的CPU利用率可以发生明显的变化，导致应用性能不一致。也就是说，CPU占用率的上界用2 σ θ m = 3328来近似，其中m是内存的大小。
我们进一步考察同时驻留实例之间如何分配CPU时间。我们设共级为同时驻留实例的个数，1的一个共级只表示VM上的单个实例。对于内存大小m，我们在2到b3328 = mc范围内选取了一个共级。然后我们对每一个同时驻留实例中的CPU利用率进行了测量。通过对20轮测试结果的分析，我们发现当前运行的实例具有几乎相同的CPU利用率( SD < 0：5 % )，它们共享CPU的能力相当。随着更多的同时驻留实例，每个实例的CPU占用率略低于，但仍接近2 * m = 3328 (在任何设置中SD < 2：5 % )。上述结果表明，AWS试图将固定的CPU周期分配给仅基于函数内存的实例
Azure和Google。谷歌采用与AWS相同的机制来分配基于功能内存的CPU循环。在谷歌，随着功能内存的增加，中值实例CPU使用率从11.1 %到100 %不等。对于给定的内存大小，不同实例之间速率的标准差非常低(图10b )，从0.62 %到2.30 %不等。Azure在CPU利用率上的方差相对较高( 14.1 % -90 % )，而中位数为66.9 %，SD为16 %。即使实例分配相同的内存量，也是如此。按vCPU编号的分解显示，4 - vCPU VM上的实例往往获得更高的CPU份额，从47 %到90 %不等(图11a )。1 - vCPUVM和2 - vCPUVM上实例的利用率分布实际上是相似的；然而，当共级增加时，1 - vCPU VM上实例的CPU利用率下降得更厉害，如图11b所示。
#### I/O 和网络
为了测量I/O吞吐率，我们在AWS和Google中的测量功能使用dd命令将512 KB的数据写入本地磁盘1000次(使用fdatasync和dsync标志确保数据写入磁盘)。在Azure中，我们使用Python脚本(使用os . fsync保证数据写入磁盘)执行同样的操作。对于网络吞吐量的测量，该函数使用具有默认配置的iperf 3.13，用不同的同区域iperf服务器运行10秒的吞吐量测试，使得iperf服务器端带宽不是瓶颈。iperf服务器使用相同类型的虚拟机作为优势点。
AWS图12显示了给定数量的同时驻留实例的I / O和网络吞吐量，平均50轮。所有同时驻留实例同时执行相同的测量。尽管总体I / O和网络吞吐量保持相对稳定，但随着共级的增加，每个实例在I / O和网络资源中所占的份额变小。当共级从1增加到20时，平均每128 MB实例的I/O吞吐率下降了4x，从13.1Mbps下降到2.9Mbps，网络吞吐量下降了19x，从538.6 MB / s下降到28.7 MB / s。
同时驻留实例以较多的争用获得较少的网络份额。我们计算每个共级的变异系数( CV )，定义为SD除以均值。较高的CV表明实例的性能差异较大。对于128 MB实例，网络吞吐量的CV在所有共级s范围内都在9 % ~ 83 %之间，这表明由于与同时驻留实例的争用而表现出明显的性能变异性。相比之下，实例之间的I / O性能相似(在所有共级中的CV为1 % -6 % )。然而，在小的内存大小(≤512 MB )情况下，I / O性能会受到功能内存( CPU )的影响，因此在与高内存大小的实例竞争时，实例的I / O吞吐量会下降更多。
Azure 在Azure中，一个实例的I / O和网络吞吐量也随着共级的增加而下降，并且由于来自其他同时驻留实例的争执而波动。更有意思的是，资源分配是根据函数实例恰好被调度在哪种类型的VM来区分的。如图12所示，与其他类型的VM相比，4 - vCPU VM可以获得1.5x高的I / O和2x高的网络吞吐量。2 - vCPU虚拟机的I/O吞吐率高于1 - vCPU虚拟机，但网络吞吐量相近。
谷歌。在谷歌，实测的I / O和网络吞吐量都随着功能内存的增加而增加：I/O吞吐率的中值范围为1.3 ~ 9.5 MB / s，网络吞吐量的中值范围为24.5 ~ 172 Mbps。相同内存大小的不同实例所测量的网络吞吐量可以有很大的差别。例如，在2 048个MB函数实例中，网络吞吐量在0.2 Mbps到321.4 Mbps之间波动。我们发现了两种情况：( 1 )所有实例吞吐量‘在给定的时间段内波动，而不论内存大小如何，或者( 2 )单个实例暂时遭受了吞吐量的退化。案例( 1 )可能是由于网络条件的变化，而案例( 2 )则导致我们怀疑GCF租户实际上共享主机并遭受资源争夺。
#### 讨论
AWS和Azure未能在同时驻留实例之间提供适当的性能隔离，因此争执会导致相当大的性能下降。在AWS中，它们将来自同一帐户的函数实例绑定到VM上，这意味着扩展函数将同一函数放置在同一VM上，导致资源争用和执行时间延长( 更不用说更长的冷启动潜伏期 )。Azure也有类似的问题，另外一个问题是VM内部的争执出现在帐户之间。后者也为跨租户退化服务攻击开辟了可能。我们离开开发新的、高效的隔离机制，把无服务器( 例如 ,频繁实例创建、短寿命实例和小内存脚印函数 )的特殊特性作为今后工作的考虑因素。
### 资源计费
在我们的研究过程中，我们发现了几个可以被租户滥用的资源会计问题。背景进程。我们在Google中发现，即使在函数调用结束后，仍然可以在后台执行外部脚本。我们运行的脚本每10秒钟向我们控制的服务器发布一个10M文件，它存活的最长时间是21小时。67相比之下，我们可以在Azure中运行这样的后台脚本，但是Azure记录了所有的活动。我们的观察表明：( 1 )在Azure和Google中，函数实例执行上下文在一次调用后不会被冻结，而不是AWS；( 2 )谷歌通过监测Node . js进程而不是整个函数实例来进行资源核算

人们可以利用谷歌的计费问题，以微不足道的成本运行复杂的任务。对于一个内存为2GB、CPU为2.4GHz的函数实例，只需支付几个调用( 0.0000029美元/ 100ms，有2M的免费调用)即可得到与在Google Cloud Platform上使用g1 -小实例( 0.0257美元/小时)相同的计算资源。Cpu会计。在谷歌，我们发现一个刚刚启动的函数实例( 2，048MB以外的任何内存大小)在时间上获得的CPU时间超过预期的80 %。通过对CPU密集型任务的CPU利用率和完成时间的测量，我们证实了一个期望CPU时间为8 % - 58 %的实例(见x6 )的CPU时间接近100 %，与给定的2 048 MB的实例相同。该实例可以保留CPU资源直到下一次调用。注意，如果你想在谷歌进行性能测量，这个问题可能会引入很多噪音

本文对三种现代无服务器计算平台的体系结构、资源利用率和性能隔离效率进行了研究。我们发现了一些问题，无论是从具体的设计决策还是从工程上提出，涉及到平台的安全性、性能和资源核算。我们的结果为今后无服务器平台设计中提高资源利用率和隔离性的研究提供了机会。