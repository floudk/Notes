### Introduction

Use RL approach to **reduce severless cold start frequency**

The serverless, or Function-as-a-Service (FaaS), paradigm suffers from function ‘cold start’ challenges, where *the serverless platform takes time to set up the dependencies, prepare the runtime environment and code for execution before serving the incoming workload*.  

Currently, most methods to solve the cold start by:

1. **reducing the start-up or preparation time** of function containers 
2. **reducing the frequency** of function cold starts on the platform  

Factors such as runtime environment, CPU and memory settings, invocation concurrency, and networking requirements, affect the cold start. The Q-learning method aims to analyze these factors so that we can detect invocation patterns and reduce cold start frequency **by preparing the function in advance**

In this Q-learning method, 

- rewards are defined by success or failure rate of response  
- environment states  are defined by the use of per-instance CPU utilization  
- actions are defined by available function instances  



无服务器计算架构提出了一种事件驱动的功能即服务模型，该模型具有细粒度的按使用付费定价，其中仅在资源使用的实际时间产生成本。这些模型定义了一组松散耦合的无状态函数（一段代码），它们在轻量级容器或虚拟机 (VM) 上执行，具有按需可扩展性的固有特征。无服务器计算完全摆脱了开发人员或用户的资源供应和管理负担，因此只强调应用程序的开发。无服务器，绝不意味着没有服务器，实际上资源管理的复杂性完全取决于云服务提供商（CSP）[13,14]。基于功能的抽象提高了应用程序开发的敏捷性，降低了管理和拥有成本。无服务器模型在轻量级函数容器内执行客户端代码，根据函数工作负载生成实例。凭借易于部署和按需功能可扩展性，Serverless 执行模型吸引了来自 IoT 服务、REST API、流处理、预测服务等多个领域的广泛应用。这些应用具有严格的延迟要求，因此期望从函数中得到快速和容错的响应。从概念上讲，无服务器架构旨在为每个传入的工作负载准备一个新实例，并在服务请求后关闭 [14]。但是，实际上，AWS Lambda、Azure Functions、Google Cloud Function 等商业无服务器平台可能会选择重用函数实例或让实例在有限的时间内运行以服务后续请求 [1]。一些在 Kubernetes 上实现的开源无服务器框架，例如 Kubeless [16] 和 Knative，具有类似的实现来保留函数的实例并重用它来服务后续请求。对于传入的工作负载，会请求新的功能容器，并且在处理请求之前会进行初始化过程。无服务器平台初始化新容器，下载客户端代码，设置代码依赖和运行环境，设置工作节点并最终执行容器来处理传入的请求。此过程引入了不可忽略的时间延迟，称为“冷启动”，并成为无服务器平台的现有挑战 [2,3,5,7]。换句话说，冷启动可以理解为平台开始执行传入请求所花费的时间。许多应用因素以及功能要求都会影响功能的冷启动。最近的研究 [6,7,8,9] 表明，编程语言、运行时环境、代码打包和部署大小、CPU 或内存要求限制等因素会影响函数的冷启动。无服务器平台的不同产品允许捕获正确的底层资源信息，并且一些开源 Kubernetes [15] 原生无服务器框架（如 Kubeless）利用了原生资源指标。为了处理函数工作负载，Kubeless 支持基于资源的自动扩展，即 Kubernetes Horizontal Pod AutoScaler (HPA) 以根据函数的每个实例的 CPU 利用率来派生新实例。仅当当前函数容器用完请求的内存或实例 CPU 利用率峰值超过指定阈值时，默认自动缩放器才会开始请求新实例。如果冷启动时间大于请求的生存时间，这会导致函数容器冷启动以服务请求，并最终导致失败的请求数量增加。由于这些观察结果完全取决于资源利用率值，因此它们为探索技术以了解过程并减少函数冷启动的频率提供了机会。

在这项工作中，我们提出了一种强化学习（RL），即一个无模型的 Q-Learning 代理，它利用每个实例的 CPU 利用率、可用函数实例的数量来表示环境状态并定义适当的奖励系统让代理动态确定给定工作负载的最佳功能实例数量。在实践中，QLearning 代理通过与无服务器环境交互来通过试错过程进行学习。在每次迭代中，代理都会分析环境的当前状态并执行特定操作。根据已实现的因素（例如 CPU 利用率、成功或失败响应）观察到延迟反馈，无论是正面的还是负面的，并因此了解工作负载模式。该策略对工作负载模式没有任何先验知识，并动态适应变化，从而降低后续调用中的冷启动频率。 该方法探索了 Q-Learning 算法在预先确定无服务器环境中函数实例的最佳数量方面的适用性，从而在特定时间跨度内减少函数冷启动的频率。 我们通过模拟 Kubeless 平台的默认自动扩展功能的工作负载模式来比较这项工作。 这有助于执行分析并检查两种配置的性能。 

我们工作的主要贡献如下： 

1. 强化学习代理实现 无服务器环境中的无模型 Q-Learning 设置以减少一个冷启动频率 功能。 
2. 实现一个代理来动态学习 确定最佳的函数调用模式 函数实例数，减少冷启动 发生。
3. 据基线评估我们提出的代理 无服务器平台的自动扩展策略 综合功能工作负载模式。 

### 相关工作

无服务器计算 - 具有可负担性、按需可扩展性和轻量级容器化，具有固有的挑战和问题。这些挑战可以概括为安全、隐私、缓存、执行模式等。其中，冷启动问题仍然普遍存在，并吸引了学术界寻求可能的解决方案。当前的一项研究 [1] 讨论了在商业和开源无服务器平台中处理冷启动的持续趋势，并通过评估 AWS Lambda 产品来展示其结果。他们将处理冷启动的方法大致分为两类：（1）优化环境，即最小化冷启动延迟本身和（2）Pinging，即最小化冷启动发生的频率。

在缓解冷启动问题的现有技术中，他们回顾了 OpenFaas、OpenWhisk、AWS Lambda 的产品，并讨论了冷和暖队列等解决方案。他们进一步创建了一个包含 I/O 密集型和 CPU 密集型基准的案例研究，用于评估 AWS Lambda 的暖队列技术，并得出结论，平台准备的暖容器与传入请求的时间间隔之间没有任何关联。在[2]中，引入了一种自适应函数容器预热技术来减少冷启动延迟。它利用函数链模型，即一系列函数来预测函数调用时间，使用 LSTM 网络和非第一个函数来保持预热的函数容器在队列中准备就绪。研究人员还提出了一种容器池策略，旨在动态调整容器池中空容器的数量，以减少资源浪费。两种方法同步工作，因为自适应预热策略失败将通过提供预热的空容器自动启动自适应容器池策略，从而减少整体冷启动延迟。研究中强调，即使该策略学习了函数链的调用时间，序列中的第一个函数也会遭受冷启动延迟。他们通过将资源利用率、空闲时间和整体集群利用率与其他现有技术进行比较来测试他们的方法。 [3] 中的研究人员针对 Knative serverless 平台解释了冷启动现象，并提出了一种 pod 迁移技术来减少函数容器的冷启动。他们假设冷启动开销取决于功能的底层实现，并将它们分类为平台相关和应用程序相关的开销。为了应对冷启动，一组预热容器，标有选择器“app-label”，准备就绪。当请求到达时，首先检查池中是否存在已预热的容器并将其分配给应用程序，否则会根据请求工作负载生成新容器。使用这种方法，他们得出结论，改进了单个池实例的容器冷启动延迟。

另一项研究 [4] 研究并利用数据相似性来减少冷启动，并提出了一种基于对等网络、虚拟文件系统和内容可寻址存储的部署系统，以提高计算能力、存储需求并防止网络瓶颈的系统。他们批评了当前从存储桶中提取每个新容器映像的容器部署技术，并在对等网络上引入了实时容器迁移技术。他们建议在需要时通过网络传输包含常用库和包的文件块，并发现容器的启动时间减少了 37.9%。同样，

[5] 旨在通过利用函数组合知识来减少冷星的数量。它提出了一种基于轻量级中间件的应用端解决方案，旨在使开发人员能够控制冷启动的频率。它确立了应用程序通常部署为一组功能，并提出了三种策略；幼稚方法、扩展方法和全局方法，其中专用编排组件调用所有步骤并遵循“提示”下一批涉及的功能的过程。 

[6] 中的研究探讨了网络创建和网络初始化是导致冷启动延迟的主要原因。它解释了容器生命周期的四个阶段：（1）服务调用，（2）启动，（3）运行时和（4）清理。清理包括停止容器、断开其网络并销毁它，这个过程需要从底层容器化守护进程循环，阻碍其他三个进程。因此，建议使用暂停容器池管理器为功能容器预先创建网络，并在需要时将新功能容器附加到配置的 IP 和网络。他们对 OpenWhisk 平台演示的评估 nstrates 减少高达 80% 的冷启动时间，而内存占用可以忽略不计。研究 [6,7,8] 已经确定了影响函数冷启动的各种因素，例如运行时环境、CPU 和内存设置、依赖设置、并发效果、网络要求等。大多数工作都集中在 AWS Lambda、Azure Functions、Google Cloud Functions 等商业无服务器平台上，而无法评估 OpenLambda、Fission、Kubeless 等开源无服务器平台。很少有研究 [9,10,11] 成功地对开源无服务器平台，并通过针对平台的容器级细粒度控制提供可能的解决方案。 [12] 中的工作将强化学习 (RL) 的范式引入了无服务器平台。它专注于在无服务器产品中基于请求的自动缩放配置 VM 或容器。该研究使用 Knative 无服务器平台进行，该平台支持利用 Kubernetes 的 Horizontal Pod Autoscaler 并行处理每个实例的请求。研究人员表明，根据工作负载，容器的不同并发级别会影响性能，因此，提出了一个基于 RL 的模型，特别是无模型 Q-Learning，以确定单个工作负载的最佳并发级别。它基于函数容器的延迟和吞吐量评估模型的性能，并展示了将 Q-Learning 算法应用于无服务器平台中的自动扩展任务的能力。作为一种新颖的方法，我们探索了 RL 策略的适用性和能力，以减少无服务器环境中的函数冷启动。与现有工作相比，我们应用无模型 Q-Learning 算法来降低无服务器平台上的冷启动频率，通过识别特定工作负载的调用模式，专注于学习函数实例的最佳数量并针对负责冷启动的非智能、默认自动缩放策略。表 1 总结了少数讨论的研究和我们的方法，突出了各个研究的区别参数。 



### 总结

无服务器计算以其易于部署的结构，使应用程序开发人员从管理服务器的职责中解脱出来。另一方面，这种执行方式增加了云服务提供商不断向客户提供容错服务的责任。由于应用程序响应时间是最终用户最重要的因素之一，无服务器引入了冷启动的开销，即为请求提供服务的函数容器的设置时间。学术界和技术行业都提出了各种方法来减少冷启动的挑战，例如保持函数容器的热队列、不断 ping 函数以保持它们运行以及保持预先准备好的具有依赖关系的容器等。这些非智能方法无法识别请求调用模式，

因此由于冷启动而导致响应失败。我们提出了在无服务器环境设置中使用强化学习技术，特别是无模型 QLearning 的证据，并提出了一种智能代理，该代理从未知的调用模式中学习，以确定函数实例的最佳数量，以降低函数的冷启动频率。

我们将我们的方法的结果与现有的自动缩放技术进行了比较，并成功地观察到，通过少量或有限数量的训练迭代，代理能够通过服务大约 50.1% 的请求来显示适度的结果。作为未来工作的一部分，我们计划使用奖励结构、α 和 γ 值以及各种函数组合的组合来扩展这种 Q-Learning 方法。我们进一步计划在智能体的学习过程中包括其他重要因素，如内存设置和函数大小等，以更好地确定状态空间中动作的关键性。由于这种方法需要对状态表示的连续值进行离散化，因此我们计划使用 DQN（深度 Q 学习网络）扩展这种方法，以估计关于最优动作的信息，而不会有状态动作空间爆炸的风险。 
